{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHT0M6cx6KO"
   },
   "source": [
    "## Connecting to S3 Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp3-5NDGxvzP"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "bucket = 'textgenerationbucket'\n",
    "key = 'text_data/Reviews.csv'\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEIFMiXO3pFt"
   },
   "outputs": [],
   "source": [
    "# Importing Nessesary Packages:\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNINU9p76TW2"
   },
   "source": [
    "## Reading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUDOd7Kn3ygV"
   },
   "outputs": [],
   "source": [
    "# Reading the Data:\n",
    "\n",
    "df = pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfcTgb3E6BIV"
   },
   "outputs": [],
   "source": [
    "data = df[['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eK2INtuD33N8"
   },
   "outputs": [],
   "source": [
    "# Making all the words to lower case:\n",
    "\n",
    "data[\"Text\"] = [re.sub(\"[^a-z' ]\", \"\", i.lower()) for i in data[\"Text\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXYL0UOw4CyA"
   },
   "outputs": [],
   "source": [
    "# Printing a sample:\n",
    "\n",
    "data[\"Text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0rIa8hn6aVB"
   },
   "source": [
    "## Creating the Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6UNp8FT4FWo"
   },
   "outputs": [],
   "source": [
    "# Function to create a sequence of length 10 Tokens:\n",
    "def create_seq(text, seq_len = 10):\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    #if the number of tokens in text is greater than 5\n",
    "    if len(text.split()) > seq_len:\n",
    "        for i in range(seq_len, len(text.split())):\n",
    "            # Select sequence of tokens\n",
    "            seq = text.split()[i-seq_len:i+1]\n",
    "            #add to the list\n",
    "            sequences.append(\" \".join(seq))\n",
    "        return sequences\n",
    "    else:\n",
    "        return[text]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_swT1IA4JN9"
   },
   "outputs": [],
   "source": [
    "sentence =\"i have bought several of the vitality canned dog food products and have found them all to be of good quality the product looks more like a stew than a processed meatand it smells better my labrador is finicky and she appreciates this product better than most.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bt3I1hQR4K2M"
   },
   "outputs": [],
   "source": [
    "create_seq(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhRHQMvN4PGg"
   },
   "outputs": [],
   "source": [
    "# Creating a list of text:\n",
    "\n",
    "seq = []\n",
    "text = data[\"Text\"].values\n",
    "for i in range(10000):\n",
    "    seqi = create_seq(text[i])\n",
    "    seq.extend([s for s in seqi if len(s.split(\" \")) == 11])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnVCecbK4S1V"
   },
   "outputs": [],
   "source": [
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tOYjZWz4U14"
   },
   "outputs": [],
   "source": [
    "for i in range(652581,652591):\n",
    "    print(seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egf5xnFH4Wwa"
   },
   "outputs": [],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seq:\n",
    "      if len(s.split()) == 11:\n",
    "        x.append(\" \".join(s.split()[:-1]))\n",
    "        y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAyB1LKX4YrM"
   },
   "outputs": [],
   "source": [
    "# Printing Last 5 Texts of  x:\n",
    "\n",
    "for i in range(652581,652591):\n",
    "    print(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAggwH8s4azb"
   },
   "outputs": [],
   "source": [
    "#Printing Last 5 Texts of y:\n",
    "\n",
    "for i in range(652581,652591):\n",
    "    print(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffMWcurx4cuO"
   },
   "outputs": [],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "for w in set(\" \".join(seq).split()):\n",
    "    int2token[cnt] = w\n",
    "    cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcB7qc3d4eht"
   },
   "outputs": [],
   "source": [
    "#Creating 2 dictionary that maps token\n",
    "\n",
    "print(token2int[\"the\"]) # Token-to-Integer\n",
    "\n",
    "print(int2token[7171])  # Integer-to-Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PqC6zi6lLW"
   },
   "source": [
    "## Saving the Dictionary as Json File to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6q5ae1br4gpW"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "dict1 = token2int\n",
    "dict2 = int2token\n",
    "s3 = boto3.resource('s3') \n",
    "obj1 = s3.Object('textgenerationbucket','inputs/token2int.json')\n",
    "obj = s3.Object('textgenerationbucket','inputs/int2token.json') \n",
    "obj1.put(Body=json.dumps(dict1))\n",
    "obj.put(Body=json.dumps(dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1XWdI1B4kRa"
   },
   "outputs": [],
   "source": [
    "# set vocabulary size\n",
    "vocab_size = len(int2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LH1dhYyD4oAo"
   },
   "outputs": [],
   "source": [
    "# converting text sequences to integer sequences:\n",
    "\n",
    "x_int = [get_integer_seq(i) for i in x]\n",
    "y_int = [get_integer_seq(i) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dS9NYGcO4ph4"
   },
   "outputs": [],
   "source": [
    "np.array(x_int).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nMOPdvX6t6D"
   },
   "source": [
    "## Saving the processed Input to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZPdJVBH4r2y"
   },
   "outputs": [],
   "source": [
    "upload_dir = 'inputs/'\n",
    "if not os.path.exists(upload_dir): # Make sure that the folder exists\n",
    "    os.makedirs(upload_dir)\n",
    "\n",
    "np.save(os.path.join(upload_dir, 'y_int.npy'), y_int)\n",
    "np.save(os.path.join(upload_dir, 'x_int.npy'), x_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3COOcjZ46zq"
   },
   "outputs": [],
   "source": [
    "# convert lists to numpy arrays\n",
    "x_int = torch.tensor(np.array(x_int))\n",
    "y_int = torch.tensor(np.array(y_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YJYCcqs5UjZ"
   },
   "outputs": [],
   "source": [
    "x_int[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkJa9Jwn63m5"
   },
   "source": [
    "## Defining the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0u4TAQm3msFm"
   },
   "outputs": [],
   "source": [
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kODPfdQr6-dN"
   },
   "source": [
    "# Making the Model Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4qSWHUm5cbY"
   },
   "outputs": [],
   "source": [
    "net = WordLSTM()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOo-Rv1x7I1O"
   },
   "source": [
    "##  Function to Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqWK4wrjm8sp"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            # inputs, targets = torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "            inputs, targets = x, y\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "            \n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AJAPUYK5hXK"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "         \n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "      # print(arr_x)\n",
    "      x = arr_x[prv:n]\n",
    "      y = arr_y[prv:n]\n",
    "      prv = n\n",
    "      yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_5L9YSI7ZYT"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfYkjf_05m1D"
   },
   "outputs": [],
   "source": [
    "train(net, batch_size = 100, epochs=20, print_every=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbA1aDzy7gXL"
   },
   "source": [
    "## Function to Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsO90utKYZtw"
   },
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(net, tkn, h=None):\n",
    "         \n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x)\n",
    "  \n",
    "  # push to GPU\n",
    "  inputs = inputs.cuda()\n",
    "\n",
    "  # detach hidden state from history\n",
    "  h = tuple([each.data for each in h])\n",
    "\n",
    "  # get the output of the model\n",
    "  print(inputs, h)\n",
    "  out, h = net(inputs)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  p = p.reshape(p.shape[1],)\n",
    "\n",
    "  # get indices of top 3 values\n",
    "  top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "  # randomly select one of the three indices\n",
    "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h\n",
    "\n",
    "\n",
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "        \n",
    "    # push to GPU\n",
    "    # net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "      token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo0kU7td7nBN"
   },
   "source": [
    "## Making the Model Predict New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0nR-DXX6P5C"
   },
   "outputs": [],
   "source": [
    "sample(net, 5, prime = \"amazing product\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sage_Maker_Text_Generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
